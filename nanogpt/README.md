# NanoGPT

## Architecture

![architecture.pnb](../asset/architecture.png)

This diagram shows the architecture we implement here. On the left is the architecture described in the "Attention is all you need" paper. We will omit the cross-attention block, so our actual architecture is shown by the diagram on the right, with one caveat: the convention now is to apply LayerNorm before the attention layer and the FFN layer, instead of after.

> *Diagram from https://www.ericjwang.com/assets/images/gpt_arch.png*

## Transformer Block

The transformer block is everything in the gray box shown in the diagram above. It consists of 4 layers:

1. LayerNorm
2. Multi-head self-attention (causal self-attention)
3. LayerNorm
4. Feed-forward network (FFN): we will use MLP

We also use residual connection and dropout layers.

### Multi-head self-attention

> Implementation: `class CausalSelfAttention` ([block.py](./block.py))

**Causal** self-attention just means each token can only attend to **previous** tokens. This is what's used in **decoder** models (e.g. GPT), as opposed to **encoder** models, where each token can attend to **all** tokens.

**Attention layer:**

This layer processes query (Q), key (K), and value (V). These vectors allow us to compute the affinity between tokens.

Every token emits these 3 vectors:
- Query: "here's what I'm interested in"
- Key: "here's what I have"
- Value: "if you find me interesting, here's what I will communicate to you"

We can think of `key(input)` and `query(input)` as "mapping each token embedding onto a key/query space." If the dot product of `q` and `k` is large, then the query and key are closely related.

If tokenA's query and tokenB's key are closely related, then we will pass tokenB's value to tokenA. We can think of this as, we use query and key to compute a "weight," then use the "weight" to determine how much of the value to pass between tokens.

**Projection layer:**

We will use a projection layer to combine the heads' results together. This introduces "mixing" instead of merely concatenating the outputs.

**Dropout:**

Dropout prevents overfitting by randomly omitting some values in the matrices (setting them to 0).

In our CausalSelfAttention block, we introduce dropout twice: once on the "weights" generated by Q and K (this is `attention_dropout`), and another time on the final output (this is `resid_dropout`). The final dropout is called `resid_dropout` because this applied right before residual connections (see where `CausalSelfAttention` is invoked in `Block` class).

### LayerNorm

> Implementation: `class LayerNorm` ([block.py](./block.py))

LayerNorm normalizes each feature vector to have mean=0 and variance=1. This helps stabilize training and prevent vanishing/exploding gradients, especially in deep networks.

```python
# Without LayerNorm:
Layer 1 output: [0.1, 0.2, 0.15]
Layer 10 output: [100, 200, 150]  # Values explode!

# With LayerNorm (after each layer):
Layer 1 output: [0.1, 0.2, 0.15] â†’ normalized
Layer 10 output: still normalized around 0 with variance 1
```

### Feed-forward network (FFN)

> Implementation: `class MLP` ([block.py](./block.py))

MLP is the most common implementation of feed-forward network. Our MLP implementation consists of 3 layers:

**Layer 1: linear layer (expansion)**

- Expands dimensions from n_embd to 4 * n_embd: (B, T, n_embd) -> (B, T, 4 * n_embd)
- "cfc" = "causally fully connected" (naming from GPT-2)
- Creates a wider hidden layer for more expressive computation

**Layer 2: GELU activation function**

- GELU = Gaussian Error Linear Unit
- Smoother / performs better than ReLU
- Allows network to learn non-linear patterns

```python
# Without activation (just linear layers):
output = W2 @ (W1 @ x) = (W2 @ W1) @ x  # Just one big linear transform!

# With activation:
output = W2 @ GELU(W1 @ x)  # Now it's non-linear!
```

**Layer 3: linear layer (projection)**

- Projects dimensions back to n_embd: (B, T, 4 * n_embd) -> (B, T, n_embd)
- Returns to original dimension to allow for residual connection

We also apply dropout on the output of the MLP block.

## Combining transformer blocks into GPT