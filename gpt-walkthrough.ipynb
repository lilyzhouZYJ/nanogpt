{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9cc124a-f8a6-4bce-9164-4d1bb6847b25",
   "metadata": {},
   "source": [
    "# NanoGPT walkthrough\n",
    "\n",
    "This notebook follows the [Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=8) video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7307acc5-4808-427b-ac75-26bd82e6b2ee",
   "metadata": {},
   "source": [
    "## Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4e6cc38-21b0-4ff4-8c96-2e8627f07827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect dataset\n",
    "with open('dataset/emma.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d453f9f-d64b-4367-93e7-1c36b1dee4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length (number of characters): 880425\n"
     ]
    }
   ],
   "source": [
    "# Dataset length\n",
    "print(f\"Dataset length (number of characters): {len(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be44ce81-a5b3-4b0f-9e89-101b0b3f8351",
   "metadata": {},
   "source": [
    "## Encoder/decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "700b2019-3ab4-4461-bb93-ec2d9be46611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: \n",
      " !&(),-.01234678:;?ABCDEFGHIJKLMNOPQRSTUVWXY[]_abcdefghijklmnopqrstuvwxyzàéêï—‘’“”\n",
      "Vocab size: 83\n"
     ]
    }
   ],
   "source": [
    "# Get vocabulary size (unique characters)\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Vocab: {''.join(chars)}\")\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07b786a9-2574-4477-b6d7-37e6b9d4b6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 48, 61, 52, 1, 48, 68, 66, 67, 52, 61]\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"jane austen\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6803e0-9739-41d7-a162-18429b1f8aa0",
   "metadata": {},
   "source": [
    "## Encode dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "489e7c3a-2378-40e3-8283-61ff3dc0efcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([880425]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "## Encode the entire dataset and store it in a torch.Tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a08e08f-01ec-4a30-8585-6720c723adf9",
   "metadata": {},
   "source": [
    "## Split dataset into `train` and `validation` sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "734a6dff-0db5-4dfc-90a3-37ce39fbf940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 880425\n",
      "Training set size: 792382\n",
      "Validation set size: 88043\n"
     ]
    }
   ],
   "source": [
    "# First 90% of dataset will be `train`; the rest is `val`\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"Dataset size: {len(data)}\")\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5ce316-c253-4a99-a227-bd92808ebfc3",
   "metadata": {},
   "source": [
    "## Context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a5a1250-fc8d-422a-a617-dc1cac2e0db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee365a09-7e25-493f-94b4-98cc674dd217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor([24]) ==> Output 60\n",
      "Input tensor([24, 60]) ==> Output 60\n",
      "Input tensor([24, 60, 60]) ==> Output 48\n",
      "Input tensor([24, 60, 60, 48]) ==> Output 0\n",
      "Input tensor([24, 60, 60, 48,  0]) ==> Output 0\n",
      "Input tensor([24, 60, 60, 48,  0,  0]) ==> Output 49\n",
      "Input tensor([24, 60, 60, 48,  0,  0, 49]) ==> Output 72\n",
      "Input tensor([24, 60, 60, 48,  0,  0, 49, 72]) ==> Output 1\n"
     ]
    }
   ],
   "source": [
    "# A block of 9 characters actually contains 8 examples\n",
    "\n",
    "# Example:\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Input {context} ==> Output {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d96d10-bc54-47e0-b603-03cd7587accc",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1bff9824-a486-4d1c-b492-6a71c3c66260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Inputs ===\n",
      "Shape: torch.Size([4, 8])\n",
      "tensor([[66,  1, 48, 61, 72, 70, 55, 52],\n",
      "        [72,  1, 48, 66,  1, 66, 55, 52],\n",
      "        [ 1, 81, 28,  1, 51, 62,  1, 61],\n",
      "        [48, 56, 66, 52,  1, 49, 52, 72]])\n",
      "=== Targets ===\n",
      "Shape: torch.Size([4, 8])\n",
      "tensor([[ 1, 48, 61, 72, 70, 55, 52, 65],\n",
      "        [ 1, 48, 66,  1, 66, 55, 52,  1],\n",
      "        [81, 28,  1, 51, 62,  1, 61, 62],\n",
      "        [56, 66, 52,  1, 49, 52, 72, 62]])\n",
      "\n",
      "=== What this means ===\n",
      "  Input [66] ==> target 1\n",
      "  Input [66, 1] ==> target 48\n",
      "  Input [66, 1, 48] ==> target 61\n",
      "  Input [66, 1, 48, 61] ==> target 72\n",
      "  Input [66, 1, 48, 61, 72] ==> target 70\n",
      "  Input [66, 1, 48, 61, 72, 70] ==> target 55\n",
      "  Input [66, 1, 48, 61, 72, 70, 55] ==> target 52\n",
      "  Input [66, 1, 48, 61, 72, 70, 55, 52] ==> target 65\n",
      "  Input [72] ==> target 1\n",
      "  Input [72, 1] ==> target 48\n",
      "  Input [72, 1, 48] ==> target 66\n",
      "  Input [72, 1, 48, 66] ==> target 1\n",
      "  Input [72, 1, 48, 66, 1] ==> target 66\n",
      "  Input [72, 1, 48, 66, 1, 66] ==> target 55\n",
      "  Input [72, 1, 48, 66, 1, 66, 55] ==> target 52\n",
      "  Input [72, 1, 48, 66, 1, 66, 55, 52] ==> target 1\n",
      "  Input [1] ==> target 81\n",
      "  Input [1, 81] ==> target 28\n",
      "  Input [1, 81, 28] ==> target 1\n",
      "  Input [1, 81, 28, 1] ==> target 51\n",
      "  Input [1, 81, 28, 1, 51] ==> target 62\n",
      "  Input [1, 81, 28, 1, 51, 62] ==> target 1\n",
      "  Input [1, 81, 28, 1, 51, 62, 1] ==> target 61\n",
      "  Input [1, 81, 28, 1, 51, 62, 1, 61] ==> target 62\n",
      "  Input [48] ==> target 56\n",
      "  Input [48, 56] ==> target 66\n",
      "  Input [48, 56, 66] ==> target 52\n",
      "  Input [48, 56, 66, 52] ==> target 1\n",
      "  Input [48, 56, 66, 52, 1] ==> target 49\n",
      "  Input [48, 56, 66, 52, 1, 49] ==> target 52\n",
      "  Input [48, 56, 66, 52, 1, 49, 52] ==> target 72\n",
      "  Input [48, 56, 66, 52, 1, 49, 52, 72] ==> target 62\n"
     ]
    }
   ],
   "source": [
    "# How many independent sequences will be processed in parallel\n",
    "batch_size = 4\n",
    "\n",
    "def get_batch(split='train', batch_size=4, block_size=8):\n",
    "    \"\"\"\n",
    "    Generates a small batch of data with inputs x and targets y\n",
    "    \"\"\"\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    # Create a tensor of randint, with shape [batch_size];\n",
    "    # this is where we start the training data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# Example batch:\n",
    "xx, yy = get_batch('train')\n",
    "print(\"=== Inputs ===\")\n",
    "print(f\"Shape: {xx.shape}\")\n",
    "print(xx)\n",
    "print(\"=== Targets ===\")\n",
    "print(f\"Shape: {yy.shape}\")\n",
    "print(yy)\n",
    "print()\n",
    "\n",
    "print('=== What this means ===')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xx[b, :t+1]\n",
    "        target = yy[b,t]\n",
    "        print(f\"  Input {context.tolist()} ==> target {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1606ebba-9ae2-41c1-9911-d0f83ca15e8f",
   "metadata": {},
   "source": [
    "## Bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5932e04a-8a0c-44e1-abb5-a84b16e639b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ca152974-e4cf-4a6f-bddc-56ee877580a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = vocab_size = 83\n",
      "B = batch_size = 32 = how many independent sequences are being processed at once\n",
      "T = time = length of the running sequence\n",
      "C = channel = 83 = size of the feature vector at each position = embedding dimension\n",
      "Right now C = vocab_size\n"
     ]
    }
   ],
   "source": [
    "# Some notes on dimensions\n",
    "print(f\"n = vocab_size = {vocab_size}\")\n",
    "print(f\"B = batch_size = {batch_size} = how many independent sequences are being processed at once\")\n",
    "print(f\"T = time = length of the running sequence\")\n",
    "print(f\"C = channel = {vocab_size} = size of the feature vector at each position = embedding dimension\")\n",
    "print(f\"Right now C = vocab_size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9096bde2-d2dc-4fe7-877e-a7004f357a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Create \"embedding\" table.\n",
    "        # - Usually a token's embedding carries semantic meaning, but in a\n",
    "        #   bigram model, it just predicts \"what comes next\".\n",
    "        # - In this lookup table, each token gets mapped to the logits of\n",
    "        #   the next token.\n",
    "        # - The lookup table is of dimension (n,n).\n",
    "        # - nn.Embedding initializes with random values\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, indices, targets=None):\n",
    "        # `indices` and `targets` are both (B,T) tensor of integers,\n",
    "\n",
    "        # For each idx in `indices`, we fetch its corresponding logits;\n",
    "        # this produces a (B,T,C) tensor\n",
    "        logits = self.token_embedding_table(indices)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # We want to flatten `logits` so that we have a total of B*T\n",
    "            # feature vectors of length C.\n",
    "            logits = logits.view(B*T, C)\n",
    "\n",
    "            # Also flatten `targets` so that it contains B*T target outputs\n",
    "            # for each of the feature vectors in `logits`.\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, indices, max_new_tokens):\n",
    "        # `indices` is a (B,T) tensor of indices in the current context\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get predictions;\n",
    "            # `logits` is (B,T,C)\n",
    "            logits, loss = self(indices) # calls forward()\n",
    "\n",
    "            # `logits` contains the logits for every index in `indices`,\n",
    "            # but we actually only need the last time step in each batch\n",
    "            logits = logits[:, -1, :] # becomes (B,C)\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "\n",
    "            # Sample from the probability distribution\n",
    "            next_idx = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "\n",
    "            # Append sampled index to the context for each batch\n",
    "            indices = torch.cat((indices, next_idx), dim=1) # (B,T+1)\n",
    "\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c861b9-9be9-4e88-ab38-864a363db8c4",
   "metadata": {},
   "source": [
    "### Run the model now without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ee894b2a-f524-4fe5-b354-b79b918f3db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([256, 83])\n",
      "Loss: tensor(5.1565, grad_fn=<NllLossBackward0>)\n",
      "Generated:\n",
      "\n",
      " p,]]4AzipzpàTBv?(1fPo1 4NW0ORnslK[NH;—Fef\n",
      "gfo3DIKwq x&LQSCPmGqvfT,;Yh’CrB)OFTJ&W!éd:YV”MT681B’z1“zE\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "# Run the model and see what it generates right now (it's not trained)\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xx, yy) # recall that xx and yy are a batch in the training set\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Loss:\", loss)\n",
    "\n",
    "# Generate some output, starting with [0]\n",
    "gen = m.generate(indices = torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)\n",
    "print(\"Generated:\")\n",
    "print(decode(gen[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8d320d-3a1a-45e0-8063-8e985c22a2d1",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8906caa3-329c-4df3-8b3b-413edba7d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "\n",
    "# PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6ca1ade6-a277-4e08-90e6-953f8f8af08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 1: 4.812380313873291\n",
      "Loss at step 5000: 2.63284969329834\n",
      "Loss at step 10000: 2.4653244018554688\n",
      "Loss at step 15000: 2.539247989654541\n",
      "Loss at step 20000: 2.39619779586792\n",
      "Loss at step 25000: 2.4903721809387207\n",
      "Loss at step 30000: 2.4283287525177\n",
      "Loss at step 35000: 2.438443183898926\n",
      "Loss at step 40000: 2.460904121398926\n",
      "Loss at step 45000: 2.4296138286590576\n",
      "Loss at step 50000: 2.4623610973358154\n"
     ]
    }
   ],
   "source": [
    "# Use bigger batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Train for some iterations\n",
    "iterations = 50000\n",
    "print_interval = 5000\n",
    "for step in range(iterations):\n",
    "    # Sample a batch of data\n",
    "    xx, yy = get_batch('train', batch_size)\n",
    "\n",
    "    # Evaluate loss\n",
    "    logits, loss = m(xx, yy)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step == 0 or step == iterations-1 or (step+1) % print_interval == 0:\n",
    "        print(f\"Loss at step {step+1}: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f33255-e037-4dfd-81ce-ad6730dedd9a",
   "metadata": {},
   "source": [
    "### Generate some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "684d1a14-c491-4fde-a5b3-a9b2ef7b96c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated:\n",
      "\n",
      "bo he atht Qu mmanine, at Mitr\n",
      "Mr bico er ol, ry I am ashetun mofesimige he frionelk f ciopind, wis t s\n",
      "asulle, e bed avequra cow se it s. Handy.”\n",
      "Perejey. ther. dfalth ved, pecom thisint t grn\n",
      "cimu w\n"
     ]
    }
   ],
   "source": [
    "# Generate some output, starting with [0]\n",
    "gen = m.generate(indices = torch.zeros((1,1), dtype=torch.long), max_new_tokens=200)\n",
    "print(\"Generated:\")\n",
    "print(decode(gen[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af6b1fa-5660-4e63-a9d7-01758a6f5e73",
   "metadata": {},
   "source": [
    "## Introducing self-attention\n",
    "\n",
    "We would like the tokens to start talking to each other.\n",
    "\n",
    "Information only flows from previous context into the future. A token cannot talk to a future token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "372925f4-d193-4d39-83ce-4d2d84970642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n",
      "tensor([[-0.5892,  0.3504],\n",
      "        [-1.8511,  1.7745],\n",
      "        [ 0.2523,  0.7422],\n",
      "        [-0.3869,  0.0273],\n",
      "        [ 1.0037, -2.0112],\n",
      "        [-0.0021,  0.2303],\n",
      "        [-1.5618, -0.1691],\n",
      "        [ 0.5601,  2.0332]])\n"
     ]
    }
   ],
   "source": [
    "# Toy example\n",
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "\n",
    "x = torch.randn(B,T,C)\n",
    "print(x.shape)\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2963d5d0-7cf0-4aab-9237-c148ff0007a6",
   "metadata": {},
   "source": [
    "### Self-attention by taking the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "30ad6642-9b5d-4578-ab6b-7f5114016284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n",
      "tensor([[-0.5892,  0.3504],\n",
      "        [-1.2201,  1.0625],\n",
      "        [-0.7293,  0.9557],\n",
      "        [-0.6437,  0.7236],\n",
      "        [-0.3142,  0.1767],\n",
      "        [-0.2622,  0.1856],\n",
      "        [-0.4479,  0.1349],\n",
      "        [-0.3219,  0.3722]])\n"
     ]
    }
   ],
   "source": [
    "# Let's start by taking just the *average* of all previous tokens + current token.\n",
    "# i.e. xbow[b,t] = mean_{i<=t} x[b,i]\n",
    "\n",
    "# xbow = x \"bag of words\"\n",
    "# \"bag of words\" just means we are just taking the average\n",
    "\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0) # average along `time` dimension => (C,)\n",
    "\n",
    "print(xbow.shape)\n",
    "print(xbow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb1f24-2e25-43aa-8d03-c0a91fbe53fd",
   "metadata": {},
   "source": [
    "### Trick using matrix multiplication\n",
    "\n",
    "We can use matrix multiplication with a `wei` array to achieve the same effect of taking the average of all previous tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "05264ff9-7cab-4e97-9f9c-b0d957332f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3c312884-f03e-45c1-a822-ec67d83fdde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = wei / wei.sum(1, keepdims=True)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ffdec951-c5d3-41bb-9c62-781e1e221b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5892,  0.3504],\n",
       "        [-1.2201,  1.0625],\n",
       "        [-0.7293,  0.9557],\n",
       "        [-0.6437,  0.7236],\n",
       "        [-0.3142,  0.1767],\n",
       "        [-0.2622,  0.1856],\n",
       "        [-0.4479,  0.1349],\n",
       "        [-0.3219,  0.3722]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2 = wei @ x\n",
    "xbow2[0]\n",
    "\n",
    "# Note on wei @ x:\n",
    "# - wei is (T,T) but x is (B,T,C)\n",
    "# - matrix multiplication will create a B dimension for wei => (B, T, T)\n",
    "# - the result will be (B,T,C)\n",
    "\n",
    "# xbow2 will be identical to xbow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243fd598-f759-4988-ae19-c83565185f27",
   "metadata": {},
   "source": [
    "### Another way by using Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9c64d09c-9828-40cc-a193-e36a1b4e13d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start by initializing `wei` as all 0's\n",
    "wei = torch.zeros((T,T))\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0eb5431f-f145-4c4a-b6bf-1d2b9678c99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f24947af-d4d9-42e1-8f97-cf46666659e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "113adbf2-0786-4358-b588-727ba02a99d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = F.softmax(wei, dim=-1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3519c108-bea3-43c0-841e-18174164ad6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5892,  0.3504],\n",
       "        [-1.2201,  1.0625],\n",
       "        [-0.7293,  0.9557],\n",
       "        [-0.6437,  0.7236],\n",
       "        [-0.3142,  0.1767],\n",
       "        [-0.2622,  0.1856],\n",
       "        [-0.4479,  0.1349],\n",
       "        [-0.3219,  0.3722]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow3 = wei @ x\n",
    "xbow3[0]\n",
    "\n",
    "# xbow3 should be identical to xbow3 and xbow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67a4046-28b4-4690-9287-21b93fa24966",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "We will use softmax because when we do `wei = wei.masked_fill(tril == 0, float('-inf'))`, we can treat `-inf` as saying \"these future tokens have no effect on the current token.\" By extension, the values before `-inf` don't all have to be 0 - these tokens can start talking to each other and take on different weights => self-attention!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c3b7a-2c96-4ec8-8bb9-7d0515cebfee",
   "metadata": {},
   "source": [
    "### Self-attention!\n",
    "\n",
    "Instead of just taking the average, we let tokens talk to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a5ae42c8-54b0-4ab7-8e6a-516de139d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8669348c-3609-4948-9c77-056e8ce3b291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previously we did\n",
    "# wei = torch.zeros((T,T))\n",
    "\n",
    "# But now we don't want this to be all uniform; instead,\n",
    "# we want to be able to gather info from the past.\n",
    "\n",
    "# Every single token at each position will emit 2 vectors: query + key.\n",
    "# - Query: \"what am I looking for\"\n",
    "# - Key: \"what do I contain\"\n",
    "# We use dot product to get affinity between tokens,\n",
    "# i.e. \"my query\" /dot \"your key\" => this becomes wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "496bda3a-e784-44ed-8144-a8d65ccd46f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Single head perform self-attention\n",
    "#\n",
    "\n",
    "head_size = 16\n",
    "\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B,T,16)\n",
    "q = query(x) # (B,T,16)\n",
    "\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "v = value(x) # (B,T,16)\n",
    "\n",
    "# We can think of k, q, and v as follows:\n",
    "# - k: \"here's what I have\"\n",
    "# - q: \"here's what I'm interested in\"\n",
    "# - v: \"if you find me interesting, here's what I will communicate to you\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c8102b67-a72d-455c-b8ce-228837eab24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1468, -0.4214,  0.6370,  0.5305,  0.2232, -0.2876, -0.0923, -0.0155],\n",
       "        [ 2.0289,  2.2027,  0.8110,  2.9605,  1.2613,  0.6308, -0.1368,  0.8316],\n",
       "        [-0.7470,  0.8219,  1.7294, -0.5071,  0.6228, -0.9951, -0.3489, -0.4836],\n",
       "        [ 0.8390,  1.4731,  1.0499,  1.2938, -0.7957, -0.4047, -0.7720,  0.2426],\n",
       "        [-0.4214,  0.5541,  1.0076,  0.1041, -0.1491, -1.3850, -0.5942,  0.1684],\n",
       "        [ 0.7089, -4.6129, -2.5074,  1.1034,  1.2286,  0.2548,  0.6364,  1.7539],\n",
       "        [ 1.4424, -1.4872, -0.2297,  0.4801,  2.0341,  0.0399, -0.1356,  0.9952],\n",
       "        [ 0.1095,  2.0939,  0.8306,  1.3074, -1.3823, -1.1200, -0.0429,  0.2455]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dot product of q and k (need to transpose the last 2 dimensions of k);\n",
    "# this is the \"affinity\" between tokens\n",
    "wei = q @ k.transpose(-2, -1) # (B,T,T)\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4e49b905-96ef-4084-adba-32e25012be3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4567, 0.5433, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0565, 0.2713, 0.6722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1756, 0.3310, 0.2168, 0.2767, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0923, 0.2449, 0.3854, 0.1562, 0.1212, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2064, 0.0010, 0.0083, 0.3062, 0.3470, 0.1311, 0.0000, 0.0000],\n",
       "        [0.2575, 0.0138, 0.0484, 0.0984, 0.4654, 0.0634, 0.0532, 0.0000],\n",
       "        [0.0619, 0.4500, 0.1272, 0.2049, 0.0139, 0.0181, 0.0531, 0.0709]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9aa81dfa-5209-43c6-913d-eda4a1c69422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instead of:\n",
    "# out = wei @ x\n",
    "\n",
    "# here we do:\n",
    "out = wei @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df8a4bb-efbf-475f-a016-b119dd0621f3",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- Each example across batch dimension is processed completely independently.\n",
    "- What we have here is a \"decoder\" attention block because it has triangular masking; this is usually used in autoregressive settings, like language modeling. There's also \"encoder\" attention block, which allows all tokens to communicate and is used in situations like sentiment analysis. In an \"encoder\" block, just remove the `tril` line that does masking.\n",
    "- \"Self-attention\" just means that the keys and values are produced from the same source as queries (i.e. `x` in our case). In \"cross-attention\", the queries still get produced from `x`, but the keys and values come from some other external source (e.g. an encoder module)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4afba57-4abf-4ab0-9e69-998ac9d92314",
   "metadata": {},
   "source": [
    "### Scaled self-attention\n",
    "\n",
    "In \"scaled\" self-attention, we further divide `wei` by **1/sqrt(head_size)**. This makes it so when input Q, K are unit variance, `wei` will be unit variance too. This ensures softmax will stay diffuse and not saturate too much (not converge towards one-hot vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e333ee28-deb1-4c17-aee2-430a1beec671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0119)\n",
      "tensor(1.0122)\n",
      "tensor(15.9142)\n"
     ]
    }
   ],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1)\n",
    "\n",
    "print(k.var())\n",
    "print(q.var())\n",
    "print(wei.var()) # head_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "25ced16a-fc4c-49b6-b280-27ebde2a60e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0559)\n",
      "tensor(0.9742)\n",
      "tensor(1.1256)\n"
     ]
    }
   ],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5\n",
    "\n",
    "print(k.var())\n",
    "print(q.var())\n",
    "print(wei.var()) # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3e658bd7-ae23-4a9f-a425-d3038a8695a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "# Why is low variance good?\n",
    "\n",
    "# low variance\n",
    "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1))\n",
    "\n",
    "# high variance:\n",
    "# this will get too peaky; convergs to one hot\n",
    "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 8, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4dc261-71e7-48fa-a810-35962d1708d6",
   "metadata": {},
   "source": [
    "## Add self-attention to `BigramLanguageModel`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51084a09-a620-4037-8b95-b074b8464cc3",
   "metadata": {},
   "source": [
    "### Add self-attention module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a121393a-7b02-4090-b324-4c5ed89d492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    One head of self-attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        # `tril` is a \"buffer\", i.e. it's not a parameter of the module.\n",
    "        # We have to call register_buffer on it.\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        \n",
    "        # Compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        \n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        # wei = self.dropout(wei)\n",
    "        \n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v     # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e74919-147a-46a5-904e-ab204eb54f96",
   "metadata": {},
   "source": [
    "### Updating BigramLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "abcc182a-c957-4fce-9b1d-5fa8bcb0378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce new variable: number of embedding dimensions\n",
    "n_embd = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5be99d5b-e938-419d-a017-0c7dd390a4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = vocab_size = 83\n",
      "B = batch_size = 32 = how many independent sequences are being processed at once\n",
      "T = time = length of the running sequence\n",
      "C = channel = 32 = size of the feature vector at each position = embedding dimension\n",
      "** C will no longer be equal to vocab_size; it will be n_embd instead **\n",
      "n_embd = 32 = number of embedding dimensions\n"
     ]
    }
   ],
   "source": [
    "# Some notes on dimensions again\n",
    "print(f\"n = vocab_size = {vocab_size}\")\n",
    "print(f\"B = batch_size = {batch_size} = how many independent sequences are being processed at once\")\n",
    "print(f\"T = time = length of the running sequence\")\n",
    "print(f\"C = channel = {n_embd} = size of the feature vector at each position = embedding dimension\")\n",
    "print(f\"** C will no longer be equal to vocab_size; it will be n_embd instead **\")\n",
    "print(f\"n_embd = {n_embd} = number of embedding dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "db9b8224-ab1e-4687-82b1-211a7b302dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Create \"embedding\" table.\n",
    "        # - Maps each token in the vocabulary to an embedding of dimension `n_embd`\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "        # New: add position embedding table\n",
    "        # - Each position in the block gets its own embedding vector\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        # New: add linear layer between embeddings (of dimension `n_embd`)\n",
    "        # and the logits (dimension `vocab_size`)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # New: self-attention head\n",
    "        self.sa_head = Head(n_embd)\n",
    "\n",
    "    def forward(self, indices, targets=None):\n",
    "        # `indices` and `targets` are both (B,T) tensor of integers,\n",
    "        B, T = indices.shape\n",
    "\n",
    "        # For each idx in `indices`, we need to fetch its corresponding logits:\n",
    "        \n",
    "        # (1) New: for each idx in `indices`, we first fetch its embedding\n",
    "        token_emb = self.token_embedding_table(indices) # (B,T,C)\n",
    "        \n",
    "        # (2) New: create the position embedding for each position in the block\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T)) # (T,C)\n",
    "\n",
    "        # (3) New: add the token embedding to position embedding\n",
    "        # - this basically means we add the embedding for each idx in `indices` to\n",
    "        #   the position embedding for its position in the block\n",
    "        # - note the dimension broadcasting here\n",
    "        x = token_emb + pos_emb  # (B,T,C)\n",
    "\n",
    "        # (4) New: apply one head of self-attention\n",
    "        x = self.sa_head(x)      # (B,T,C)\n",
    "        \n",
    "        # (5) New: we then fetch the logits using the lm_head layer\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # We want to flatten `logits` so that we have a total of B*T\n",
    "            # feature vectors of length C.\n",
    "            logits = logits.view(B*T, C)\n",
    "\n",
    "            # Also flatten `targets` so that it contains B*T target outputs\n",
    "            # for each of the feature vectors in `logits`.\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, indices, max_new_tokens):\n",
    "        # `indices` is a (B,T) tensor of indices in the current context\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # New: we need to crop the context; otherwise it won't\n",
    "            # fit into our position_embedding_table\n",
    "            indices_cropped = indices[:, -block_size:]\n",
    "            \n",
    "            # Get predictions;\n",
    "            # `logits` is (B,T,C)\n",
    "            logits, loss = self(indices_cropped) # calls forward()\n",
    "\n",
    "            # `logits` contains the logits for every index in `indices`,\n",
    "            # but we actually only need the last time step in each batch\n",
    "            logits = logits[:, -1, :] # becomes (B,C)\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "\n",
    "            # Sample from the probability distribution\n",
    "            next_idx = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "\n",
    "            # Append sampled index to the context for each batch\n",
    "            indices = torch.cat((indices, next_idx), dim=1) # (B,T+1)\n",
    "\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce7c722-3af0-4a1b-9919-594ea062f733",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "57a4167f-dd34-40b6-9091-af7a51f010a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "\n",
    "# PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e48b8c80-1f8e-4856-ace7-9efdf1d81871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 1: 4.578811168670654\n",
      "Loss at step 5000: 2.384617567062378\n",
      "Loss at step 10000: 2.3340530395507812\n",
      "Loss at step 15000: 2.326909065246582\n",
      "Loss at step 20000: 2.2604215145111084\n",
      "Loss at step 25000: 2.27081561088562\n",
      "Loss at step 30000: 2.4316458702087402\n",
      "Loss at step 35000: 2.3086352348327637\n",
      "Loss at step 40000: 2.2617619037628174\n",
      "Loss at step 45000: 2.273874044418335\n",
      "Loss at step 50000: 2.303931474685669\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Train for some iterations\n",
    "iterations = 50000\n",
    "print_interval = 5000\n",
    "for step in range(iterations):\n",
    "    # Sample a batch of data\n",
    "    xx, yy = get_batch('train', batch_size)\n",
    "\n",
    "    # Evaluate loss\n",
    "    logits, loss = m(xx, yy)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step == 0 or step == iterations-1 or (step+1) % print_interval == 0:\n",
    "        print(f\"Loss at step {step+1}: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85f750f-8e5a-43fb-ab2b-90ca691b0f04",
   "metadata": {},
   "source": [
    "### Generate some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "aff969f5-12d6-466b-be98-ee62dbea5d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated:\n",
      "\n",
      "“No, at tud,” and, was, as ur onok yon ta thearsoret ngt wanto was to bus wer, bellid wingth Jato ala sen mof\n",
      "med laditoe beed. Weten. “Hairse\n",
      "seak. \n",
      "buse marsurse\n",
      "ssobyifitudm\n",
      "t\n",
      "uchas the serfir hill\n"
     ]
    }
   ],
   "source": [
    "# Generate some output, starting with [0]\n",
    "gen = m.generate(indices = torch.zeros((1,1), dtype=torch.long), max_new_tokens=200)\n",
    "print(\"Generated:\")\n",
    "print(decode(gen[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3139825b-8910-4ca9-9302-c0c53c967434",
   "metadata": {},
   "source": [
    "## Adding multi-head self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ec81a84c-295c-4ec7-83d2-37e763afef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple heads of self-attention in parallel.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        # Create multiple heads\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Concatenate the result of each head\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1) # concatenate over the channel dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "0dc91713-fd70-48fe-8496-7f16eaba48ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Create \"embedding\" table.\n",
    "        # - Maps each token in the vocabulary to an embedding of dimension `n_embd`\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "        # Add position embedding table\n",
    "        # - Each position in the block gets its own embedding vector\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        # Add linear layer between embeddings (of dimension `n_embd`)\n",
    "        # and the logits (dimension `vocab_size`)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # New: self-attention heads\n",
    "        # i.e. 4 heads of 8-dimensional self-attention\n",
    "        self.sa_heads = MultiHeadAttention(4, n_embd // 4)\n",
    "\n",
    "    def forward(self, indices, targets=None):\n",
    "        # `indices` and `targets` are both (B,T) tensor of integers\n",
    "        B, T = indices.shape\n",
    "\n",
    "        # For each idx in `indices`, we need to fetch its corresponding logits:\n",
    "        \n",
    "        # (1) For each idx in `indices`, we first fetch its embedding\n",
    "        token_emb = self.token_embedding_table(indices) # (B,T,C)\n",
    "        \n",
    "        # (2) Create the position embedding for each position in the block\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T)) # (T,C)\n",
    "\n",
    "        # (3) Add the token embedding to position embedding\n",
    "        # - this basically means we add the embedding for each idx in `indices` to\n",
    "        #   the position embedding for its position in the block\n",
    "        # - note the dimension broadcasting here\n",
    "        x = token_emb + pos_emb  # (B,T,C)\n",
    "\n",
    "        # (4) New: apply multi-head self-attention\n",
    "        x = self.sa_heads(x)      # (B,T,C)\n",
    "        \n",
    "        # (5) We then fetch the logits using the lm_head layer\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # We want to flatten `logits` so that we have a total of B*T\n",
    "            # feature vectors of length C.\n",
    "            logits = logits.view(B*T, C)\n",
    "\n",
    "            # Also flatten `targets` so that it contains B*T target outputs\n",
    "            # for each of the feature vectors in `logits`.\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, indices, max_new_tokens):\n",
    "        # `indices` is a (B,T) tensor of indices in the current context\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # We need to crop the context; otherwise it won't\n",
    "            # fit into our position_embedding_table\n",
    "            indices_cropped = indices[:, -block_size:]\n",
    "            \n",
    "            # Get predictions;\n",
    "            # `logits` is (B,T,C)\n",
    "            logits, loss = self(indices_cropped) # calls forward()\n",
    "\n",
    "            # `logits` contains the logits for every index in `indices`,\n",
    "            # but we actually only need the last time step in each batch\n",
    "            logits = logits[:, -1, :] # becomes (B,C)\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "\n",
    "            # Sample from the probability distribution\n",
    "            next_idx = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "\n",
    "            # Append sampled index to the context for each batch\n",
    "            indices = torch.cat((indices, next_idx), dim=1) # (B,T+1)\n",
    "\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1915ae6-8833-4f33-a10d-a7b4ce52963f",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "905f9c5b-f059-40e3-9a8e-4a6599bab049",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "\n",
    "# PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "73a1e616-c175-4fcf-a9ed-ffd1b7f76b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 1: 4.431545257568359\n",
      "Loss at step 5000: 2.4055771827697754\n",
      "Loss at step 10000: 2.2029716968536377\n",
      "Loss at step 15000: 2.145434617996216\n",
      "Loss at step 20000: 2.032653570175171\n",
      "Loss at step 25000: 2.0246944427490234\n",
      "Loss at step 30000: 1.9365017414093018\n",
      "Loss at step 35000: 2.381767511367798\n",
      "Loss at step 40000: 2.0720951557159424\n",
      "Loss at step 45000: 1.8479046821594238\n",
      "Loss at step 50000: 1.978032112121582\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Train for some iterations\n",
    "iterations = 50000\n",
    "print_interval = 5000\n",
    "for step in range(iterations):\n",
    "    # Sample a batch of data\n",
    "    xx, yy = get_batch('train', batch_size)\n",
    "\n",
    "    # Evaluate loss\n",
    "    logits, loss = m(xx, yy)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step == 0 or step == iterations-1 or (step+1) % print_interval == 0:\n",
    "        print(f\"Loss at step {step+1}: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd36270b-cba7-4fcd-8bfa-05a44656e3a7",
   "metadata": {},
   "source": [
    "### Generate some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e4df8af8-51ce-4b38-8db0-0703b8d1fdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated:\n",
      "\n",
      "whathery the\n",
      "soutles htime. Emma. Emmake, thoust ewng\n",
      "att to to reljeser ther being gively caratill amorselway\n",
      "thoust hermith inf willy mus.”\n",
      "\n",
      "“He reas had of an tokes the being. Henif ith anvaning, e\n"
     ]
    }
   ],
   "source": [
    "# Generate some output, starting with [0]\n",
    "gen = m.generate(indices = torch.zeros((1,1), dtype=torch.long), max_new_tokens=200)\n",
    "print(\"Generated:\")\n",
    "print(decode(gen[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d452d1cb-adcf-4ac0-82a0-38eaf742991d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
