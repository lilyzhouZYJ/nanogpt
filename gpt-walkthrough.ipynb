{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9cc124a-f8a6-4bce-9164-4d1bb6847b25",
   "metadata": {},
   "source": [
    "# NanoGPT walkthrough\n",
    "\n",
    "This notebook follows the [Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=8) video.\n",
    "\n",
    "Helpful resources on attention:\n",
    "- [Attention in transformers - 3Blue1Brown](https://www.youtube.com/watch?v=eMlx5fFNoYc&t=269s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7307acc5-4808-427b-ac75-26bd82e6b2ee",
   "metadata": {},
   "source": [
    "## Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e6cc38-21b0-4ff4-8c96-2e8627f07827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect dataset\n",
    "with open('dataset/emma.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d453f9f-d64b-4367-93e7-1c36b1dee4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length (number of characters): 880425\n"
     ]
    }
   ],
   "source": [
    "# Dataset length\n",
    "print(f\"Dataset length (number of characters): {len(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be44ce81-a5b3-4b0f-9e89-101b0b3f8351",
   "metadata": {},
   "source": [
    "## Encoder/decoder\n",
    "\n",
    "- Encoder: encode string into indices\n",
    "- Decoder: decode indices into string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "700b2019-3ab4-4461-bb93-ec2d9be46611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: \n",
      " !&(),-.01234678:;?ABCDEFGHIJKLMNOPQRSTUVWXY[]_abcdefghijklmnopqrstuvwxyzàéêï—‘’“”\n",
      "Vocab size: 83\n"
     ]
    }
   ],
   "source": [
    "# Get vocabulary size (unique characters)\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Vocab: {''.join(chars)}\")\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07b786a9-2574-4477-b6d7-37e6b9d4b6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 48, 61, 52, 1, 48, 68, 66, 67, 52, 61]\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"jane austen\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6803e0-9739-41d7-a162-18429b1f8aa0",
   "metadata": {},
   "source": [
    "## Encode dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "489e7c3a-2378-40e3-8283-61ff3dc0efcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([880425]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "## Encode the entire dataset and store it in a torch.Tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a08e08f-01ec-4a30-8585-6720c723adf9",
   "metadata": {},
   "source": [
    "## Split dataset into `train` and `validation` sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "734a6dff-0db5-4dfc-90a3-37ce39fbf940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 880425\n",
      "Training set size: 792382\n",
      "Validation set size: 88043\n"
     ]
    }
   ],
   "source": [
    "# First 90% of dataset will be `train`; the rest is `val`\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"Dataset size: {len(data)}\")\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5ce316-c253-4a99-a227-bd92808ebfc3",
   "metadata": {},
   "source": [
    "## Context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a5a1250-fc8d-422a-a617-dc1cac2e0db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee365a09-7e25-493f-94b4-98cc674dd217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor([24]) ==> Output 60\n",
      "Input tensor([24, 60]) ==> Output 60\n",
      "Input tensor([24, 60, 60]) ==> Output 48\n",
      "Input tensor([24, 60, 60, 48]) ==> Output 0\n",
      "Input tensor([24, 60, 60, 48,  0]) ==> Output 0\n",
      "Input tensor([24, 60, 60, 48,  0,  0]) ==> Output 49\n",
      "Input tensor([24, 60, 60, 48,  0,  0, 49]) ==> Output 72\n",
      "Input tensor([24, 60, 60, 48,  0,  0, 49, 72]) ==> Output 1\n"
     ]
    }
   ],
   "source": [
    "# A block of 9 characters actually contains 8 examples\n",
    "\n",
    "# Example:\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Input {context} ==> Output {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d96d10-bc54-47e0-b603-03cd7587accc",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bff9824-a486-4d1c-b492-6a71c3c66260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Inputs ===\n",
      "Shape: torch.Size([4, 8])\n",
      "tensor([[66, 66,  1, 48, 61, 51,  1, 66],\n",
      "        [55, 52, 48, 51,  1, 48,  1, 59],\n",
      "        [55, 56, 60,  1, 61, 62, 67,  1],\n",
      "        [ 1, 56, 67,  8,  1, 28,  1, 51]])\n",
      "=== Targets ===\n",
      "Shape: torch.Size([4, 8])\n",
      "tensor([[66,  1, 48, 61, 51,  1, 66, 48],\n",
      "        [52, 48, 51,  1, 48,  1, 59, 56],\n",
      "        [56, 60,  1, 61, 62, 67,  1, 67],\n",
      "        [56, 67,  8,  1, 28,  1, 51, 62]])\n",
      "\n",
      "=== What this means ===\n",
      "  Input [66] ==> target 66\n",
      "  Input [66, 66] ==> target 1\n",
      "  Input [66, 66, 1] ==> target 48\n",
      "  Input [66, 66, 1, 48] ==> target 61\n",
      "  Input [66, 66, 1, 48, 61] ==> target 51\n",
      "  Input [66, 66, 1, 48, 61, 51] ==> target 1\n",
      "  Input [66, 66, 1, 48, 61, 51, 1] ==> target 66\n",
      "  Input [66, 66, 1, 48, 61, 51, 1, 66] ==> target 48\n",
      "  Input [55] ==> target 52\n",
      "  Input [55, 52] ==> target 48\n",
      "  Input [55, 52, 48] ==> target 51\n",
      "  Input [55, 52, 48, 51] ==> target 1\n",
      "  Input [55, 52, 48, 51, 1] ==> target 48\n",
      "  Input [55, 52, 48, 51, 1, 48] ==> target 1\n",
      "  Input [55, 52, 48, 51, 1, 48, 1] ==> target 59\n",
      "  Input [55, 52, 48, 51, 1, 48, 1, 59] ==> target 56\n",
      "  Input [55] ==> target 56\n",
      "  Input [55, 56] ==> target 60\n",
      "  Input [55, 56, 60] ==> target 1\n",
      "  Input [55, 56, 60, 1] ==> target 61\n",
      "  Input [55, 56, 60, 1, 61] ==> target 62\n",
      "  Input [55, 56, 60, 1, 61, 62] ==> target 67\n",
      "  Input [55, 56, 60, 1, 61, 62, 67] ==> target 1\n",
      "  Input [55, 56, 60, 1, 61, 62, 67, 1] ==> target 67\n",
      "  Input [1] ==> target 56\n",
      "  Input [1, 56] ==> target 67\n",
      "  Input [1, 56, 67] ==> target 8\n",
      "  Input [1, 56, 67, 8] ==> target 1\n",
      "  Input [1, 56, 67, 8, 1] ==> target 28\n",
      "  Input [1, 56, 67, 8, 1, 28] ==> target 1\n",
      "  Input [1, 56, 67, 8, 1, 28, 1] ==> target 51\n",
      "  Input [1, 56, 67, 8, 1, 28, 1, 51] ==> target 62\n"
     ]
    }
   ],
   "source": [
    "# How many independent sequences will be processed in parallel\n",
    "batch_size = 4\n",
    "\n",
    "def get_batch(split='train', batch_size=4, block_size=8):\n",
    "    \"\"\"\n",
    "    Generates a small batch of data with inputs x and targets y\n",
    "    \"\"\"\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    # Create a tensor of randint, with shape [batch_size];\n",
    "    # this is where we start the training data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# Example batch:\n",
    "xx, yy = get_batch('train')\n",
    "print(\"=== Inputs ===\")\n",
    "print(f\"Shape: {xx.shape}\")\n",
    "print(xx)\n",
    "print(\"=== Targets ===\")\n",
    "print(f\"Shape: {yy.shape}\")\n",
    "print(yy)\n",
    "print()\n",
    "\n",
    "print('=== What this means ===')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xx[b, :t+1]\n",
    "        target = yy[b,t]\n",
    "        print(f\"  Input {context.tolist()} ==> target {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1606ebba-9ae2-41c1-9911-d0f83ca15e8f",
   "metadata": {},
   "source": [
    "## Bigram model\n",
    "\n",
    "We start off with a simple bigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5932e04a-8a0c-44e1-abb5-a84b16e639b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca152974-e4cf-4a6f-bddc-56ee877580a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = vocab_size = 83\n",
      "B = batch_size = 4 = how many independent sequences are being processed at once\n",
      "T = time = length of the running sequence\n",
      "C = channel = 83 = size of the feature vector at each position = embedding dimension\n",
      "Right now C = vocab_size\n"
     ]
    }
   ],
   "source": [
    "# Some notes on dimensions\n",
    "print(f\"n = vocab_size = {vocab_size}\")\n",
    "print(f\"B = batch_size = {batch_size} = how many independent sequences are being processed at once\")\n",
    "print(f\"T = time = length of the running sequence\")\n",
    "print(f\"C = channel = {vocab_size} = size of the feature vector at each position = embedding dimension\")\n",
    "print(f\"Right now C = vocab_size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9096bde2-d2dc-4fe7-877e-a7004f357a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Create \"embedding\" table.\n",
    "        # - Usually a token's embedding carries semantic meaning, but in a\n",
    "        #   bigram model, it just predicts \"what comes next\".\n",
    "        # - In this lookup table, each token gets mapped to the logits of\n",
    "        #   the next token.\n",
    "        # - The lookup table is of dimension (n,n).\n",
    "        # - nn.Embedding initializes with random values\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, indices, targets=None):\n",
    "        # `indices` and `targets` are both (B,T) tensor of integers,\n",
    "\n",
    "        # For each idx in `indices`, we fetch its corresponding logits;\n",
    "        # this produces a (B,T,C) tensor\n",
    "        logits = self.token_embedding_table(indices)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # We want to flatten `logits` so that we have a total of B*T\n",
    "            # feature vectors of length C.\n",
    "            logits = logits.view(B*T, C)\n",
    "\n",
    "            # Also flatten `targets` so that it contains B*T target outputs\n",
    "            # for each of the feature vectors in `logits`.\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, indices, max_new_tokens):\n",
    "        # `indices` is a (B,T) tensor of indices in the current context\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get predictions;\n",
    "            # `logits` is (B,T,C)\n",
    "            logits, loss = self(indices) # calls forward()\n",
    "\n",
    "            # `logits` contains the logits for every index in `indices`,\n",
    "            # but we actually only need the last time step in each batch\n",
    "            logits = logits[:, -1, :] # becomes (B,C)\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "\n",
    "            # Sample from the probability distribution\n",
    "            next_idx = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "\n",
    "            # Append sampled index to the context for each batch\n",
    "            indices = torch.cat((indices, next_idx), dim=1) # (B,T+1)\n",
    "\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c861b9-9be9-4e88-ab38-864a363db8c4",
   "metadata": {},
   "source": [
    "### Run the model now without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee894b2a-f524-4fe5-b354-b79b918f3db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([32, 83])\n",
      "Loss: tensor(5.1510, grad_fn=<NllLossBackward0>)\n",
      "Generated:\n",
      "\n",
      "NIïI”;t‘o)Q;cihE,Qp;)xdE-xtmk 4WYgIà—MjU;TU2jT,YuU]g’xb!-Yée‘U2YLcH4-éV3Bqs6P,8N(J;?;gQ”;cn)kw T&iVl\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "# Run the model and see what it generates right now (it's not trained)\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xx, yy) # recall that xx and yy are a batch in the training set\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Loss:\", loss)\n",
    "\n",
    "# Generate some output, starting with [0]\n",
    "gen = m.generate(indices = torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)\n",
    "print(\"Generated:\")\n",
    "print(decode(gen[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8d320d-3a1a-45e0-8063-8e985c22a2d1",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8906caa3-329c-4df3-8b3b-413edba7d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "\n",
    "# PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ca1ade6-a277-4e08-90e6-953f8f8af08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 1: 5.069802761077881\n",
      "Loss at step 5000: 2.678922176361084\n",
      "Loss at step 10000: 2.4121274948120117\n",
      "Loss at step 15000: 2.4237539768218994\n",
      "Loss at step 20000: 2.4447739124298096\n",
      "Loss at step 25000: 2.481267213821411\n",
      "Loss at step 30000: 2.442058563232422\n",
      "Loss at step 35000: 2.5376250743865967\n",
      "Loss at step 40000: 2.493802785873413\n",
      "Loss at step 45000: 2.3820197582244873\n",
      "Loss at step 50000: 2.394484281539917\n"
     ]
    }
   ],
   "source": [
    "# Use bigger batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Train for some iterations\n",
    "iterations = 50000\n",
    "print_interval = 5000\n",
    "for step in range(iterations):\n",
    "    # Sample a batch of data\n",
    "    xx, yy = get_batch('train', batch_size)\n",
    "\n",
    "    # Evaluate loss\n",
    "    logits, loss = m(xx, yy)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step == 0 or step == iterations-1 or (step+1) % print_interval == 0:\n",
    "        print(f\"Loss at step {step+1}: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f33255-e037-4dfd-81ce-ad6730dedd9a",
   "metadata": {},
   "source": [
    "### Generate some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "684d1a14-c491-4fde-a5b3-a9b2ef7b96c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated:\n",
      "\n",
      "cr swas. win Balddwhe w. ad. ar! icetist be podeshe s aste aly, iss (eat ithelerend d heverorodliloulsh!—Shenqucer\n",
      "m, te ppad ghef I l to nk\n",
      "Mr; Emongelouathathamood aldinerso ws l chitlelemusertlell. oread dlsth am algatire blkeman. ood our ees w my_ustonod tr ame hilin, vesth st, mul minofofeand s. condaren monen helive tcio o shil Fancr burertous d ales ald he\n",
      "hacoty avef\n",
      " chitie a? orr; a I lffalapt Shath!—‘Wemawallof e peissby head tebee, mestilld indepeenthed oro Ing se spe whino ryout bth\n"
     ]
    }
   ],
   "source": [
    "# Generate some output, starting with [0]\n",
    "gen = m.generate(indices = torch.zeros((1,1), dtype=torch.long), max_new_tokens=500)\n",
    "print(\"Generated:\")\n",
    "print(decode(gen[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af6b1fa-5660-4e63-a9d7-01758a6f5e73",
   "metadata": {},
   "source": [
    "## Introducing self-attention\n",
    "\n",
    "We would like the tokens to start talking to each other.\n",
    "\n",
    "Information only flows from previous context into the future. A token cannot talk to a future token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "372925f4-d193-4d39-83ce-4d2d84970642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n",
      "tensor([[ 0.2809, -0.6028],\n",
      "        [-0.8396, -0.4434],\n",
      "        [ 0.5945,  1.3260],\n",
      "        [-1.2213, -0.6181],\n",
      "        [-0.1123, -0.2627],\n",
      "        [ 0.2901, -0.6130],\n",
      "        [-0.8300,  0.0629],\n",
      "        [ 0.0556,  0.2140]])\n"
     ]
    }
   ],
   "source": [
    "# Toy example\n",
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "\n",
    "x = torch.randn(B,T,C)\n",
    "print(x.shape)\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2963d5d0-7cf0-4aab-9237-c148ff0007a6",
   "metadata": {},
   "source": [
    "### Self-attention by taking the average of the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30ad6642-9b5d-4578-ab6b-7f5114016284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n",
      "tensor([[ 0.2809, -0.6028],\n",
      "        [-0.2794, -0.5231],\n",
      "        [ 0.0119,  0.0933],\n",
      "        [-0.2964, -0.0846],\n",
      "        [-0.2596, -0.1202],\n",
      "        [-0.1680, -0.2023],\n",
      "        [-0.2625, -0.1645],\n",
      "        [-0.2228, -0.1172]])\n"
     ]
    }
   ],
   "source": [
    "# Let's start by taking just the *average* of all previous tokens + current token.\n",
    "# i.e. xbow[b,t] = mean_{i<=t} x[b,i]\n",
    "\n",
    "# xbow = x \"bag of words\"\n",
    "# \"bag of words\" just means we are just taking the average\n",
    "\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0) # average along `time` dimension => (C,)\n",
    "\n",
    "print(xbow.shape)\n",
    "print(xbow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb1f24-2e25-43aa-8d03-c0a91fbe53fd",
   "metadata": {},
   "source": [
    "### Trick using matrix multiplication\n",
    "\n",
    "We can use matrix multiplication with a `wei` array to achieve the same effect of taking the average of all previous tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05264ff9-7cab-4e97-9f9c-b0d957332f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c312884-f03e-45c1-a822-ec67d83fdde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = wei / wei.sum(1, keepdims=True)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffdec951-c5d3-41bb-9c62-781e1e221b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2809, -0.6028],\n",
       "        [-0.2794, -0.5231],\n",
       "        [ 0.0119,  0.0933],\n",
       "        [-0.2964, -0.0846],\n",
       "        [-0.2596, -0.1202],\n",
       "        [-0.1680, -0.2023],\n",
       "        [-0.2625, -0.1645],\n",
       "        [-0.2228, -0.1172]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2 = wei @ x\n",
    "xbow2[0]\n",
    "\n",
    "# Note on wei @ x:\n",
    "# - wei is (T,T) but x is (B,T,C)\n",
    "# - matrix multiplication will create a B dimension for wei => (B, T, T)\n",
    "# - the result will be (B,T,C)\n",
    "\n",
    "# xbow2 will be identical to xbow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243fd598-f759-4988-ae19-c83565185f27",
   "metadata": {},
   "source": [
    "### Another way by using Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c64d09c-9828-40cc-a193-e36a1b4e13d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start by initializing `wei` as all 0's\n",
    "wei = torch.zeros((T,T))\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0eb5431f-f145-4c4a-b6bf-1d2b9678c99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f24947af-d4d9-42e1-8f97-cf46666659e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "113adbf2-0786-4358-b588-727ba02a99d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = F.softmax(wei, dim=-1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3519c108-bea3-43c0-841e-18174164ad6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2809, -0.6028],\n",
       "        [-0.2794, -0.5231],\n",
       "        [ 0.0119,  0.0933],\n",
       "        [-0.2964, -0.0846],\n",
       "        [-0.2596, -0.1202],\n",
       "        [-0.1680, -0.2023],\n",
       "        [-0.2625, -0.1645],\n",
       "        [-0.2228, -0.1172]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow3 = wei @ x\n",
    "xbow3[0]\n",
    "\n",
    "# xbow3 should be identical to xbow3 and xbow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67a4046-28b4-4690-9287-21b93fa24966",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "We will use softmax because when we do `wei = wei.masked_fill(tril == 0, float('-inf'))`, we can treat `-inf` as saying \"these future tokens have no effect on the current token.\" By extension, the values before `-inf` don't all have to be 0 - these tokens can start talking to each other and take on different weights => self-attention!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c3b7a-2c96-4ec8-8bb9-7d0515cebfee",
   "metadata": {},
   "source": [
    "### Self-attention!\n",
    "\n",
    "Instead of just taking the average, we let tokens talk to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5ae42c8-54b0-4ab7-8e6a-516de139d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8669348c-3609-4948-9c77-056e8ce3b291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previously we did\n",
    "# wei = torch.zeros((T,T))\n",
    "\n",
    "# But now we don't want this to be all uniform; instead,\n",
    "# we want to be able to gather info from the past.\n",
    "\n",
    "# Every single token at each position will emit 2 vectors: query + key.\n",
    "# - Query: \"what am I looking for\"\n",
    "# - Key: \"what do I contain\"\n",
    "# We use dot product to get affinity between tokens,\n",
    "# i.e. \"my query\" /dot \"your key\" => this becomes wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "496bda3a-e784-44ed-8144-a8d65ccd46f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Single head perform self-attention\n",
    "#\n",
    "\n",
    "head_size = 16\n",
    "\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B,T,16)\n",
    "q = query(x) # (B,T,16)\n",
    "\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "v = value(x) # (B,T,16)\n",
    "\n",
    "# We can think of k, q, and v as follows:\n",
    "# - k: \"here's what I have\"\n",
    "# - q: \"here's what I'm interested in\"\n",
    "# - v: \"if you find me interesting, here's what I will communicate to you\"\n",
    "\n",
    "# We can think of key(x) and query(x) as \"mapping each embedding\n",
    "# onto a query/key space.\" If the dot product of q and k is large,\n",
    "# then the query and key are closely related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8102b67-a72d-455c-b8ce-228837eab24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4602,  0.5454,  0.4547,  1.7431,  0.6014,  0.2491,  0.1107,  1.6866],\n",
       "        [ 0.3623, -0.1856, -0.9913, -0.2403, -1.7746,  0.9635, -0.0194, -2.5452],\n",
       "        [ 0.2473, -0.9395, -0.8748, -2.2320, -0.4213, -0.0547,  0.6652,  0.1099],\n",
       "        [-0.3361,  1.7243, -0.0799,  1.2426, -0.2039,  1.9121, -0.8425, -0.5857],\n",
       "        [ 0.3423, -0.3405, -0.4143, -0.5832, -0.7255,  0.9499, -0.6766, -0.9575],\n",
       "        [-0.3074,  0.4796,  0.0699, -0.3749,  0.1905, -0.7777,  0.2963,  0.4356],\n",
       "        [-0.2227,  2.2977, -1.1799, -0.9838, -1.4433,  0.7707, -0.1478, -0.0225],\n",
       "        [-1.9678,  1.7853, -0.3128, -0.1258,  0.1161, -0.0888,  0.0289, -0.2981]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dot product of q and k (need to transpose the last 2 dimensions of k);\n",
    "# this is the \"affinity\" between tokens\n",
    "wei = q @ k.transpose(-2, -1) # (B,T,T)\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e49b905-96ef-4084-adba-32e25012be3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6336, 0.3664, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6132, 0.1871, 0.1997, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0667, 0.5236, 0.0862, 0.3235, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3684, 0.1861, 0.1729, 0.1460, 0.1266, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1272, 0.2795, 0.1855, 0.1189, 0.2093, 0.0795, 0.0000, 0.0000],\n",
       "        [0.0545, 0.6773, 0.0209, 0.0254, 0.0161, 0.1471, 0.0587, 0.0000],\n",
       "        [0.0121, 0.5173, 0.0635, 0.0765, 0.0975, 0.0794, 0.0893, 0.0644]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9aa81dfa-5209-43c6-913d-eda4a1c69422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instead of:\n",
    "# out = wei @ x\n",
    "\n",
    "# here we do:\n",
    "out = wei @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df8a4bb-efbf-475f-a016-b119dd0621f3",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- Each example across batch dimension is processed completely independently.\n",
    "- What we have here is a \"decoder\" attention block because it has triangular masking; this is usually used in autoregressive settings, like language modeling. There's also \"encoder\" attention block, which allows all tokens to communicate and is used in situations like sentiment analysis. In an \"encoder\" block, just remove the `tril` line that does masking.\n",
    "- \"Self-attention\" just means that the keys and values are produced from the same source as queries (i.e. `x` in our case). In \"cross-attention\", the queries still get produced from `x`, but the keys and values come from some other external source (e.g. an encoder module)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4afba57-4abf-4ab0-9e69-998ac9d92314",
   "metadata": {},
   "source": [
    "### Scaled self-attention\n",
    "\n",
    "In \"scaled\" self-attention, we further divide `wei` by **1/sqrt(head_size)**. This makes it so when input Q, K are unit variance, `wei` will be unit variance too. This ensures softmax will stay diffuse and not saturate too much (not converge towards one-hot vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e333ee28-deb1-4c17-aee2-430a1beec671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9121)\n",
      "tensor(1.0475)\n",
      "tensor(16.3956)\n"
     ]
    }
   ],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1)\n",
    "\n",
    "print(k.var())\n",
    "print(q.var())\n",
    "print(wei.var()) # head_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25ced16a-fc4c-49b6-b280-27ebde2a60e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0559)\n",
      "tensor(0.9348)\n",
      "tensor(0.9975)\n"
     ]
    }
   ],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5\n",
    "\n",
    "print(k.var())\n",
    "print(q.var())\n",
    "print(wei.var()) # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e658bd7-ae23-4a9f-a425-d3038a8695a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "# Why is low variance good?\n",
    "\n",
    "# low variance\n",
    "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1))\n",
    "\n",
    "# high variance:\n",
    "# this will get too peaky; convergs to one hot\n",
    "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 8, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4dc261-71e7-48fa-a810-35962d1708d6",
   "metadata": {},
   "source": [
    "## Add self-attention to our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51084a09-a620-4037-8b95-b074b8464cc3",
   "metadata": {},
   "source": [
    "### Add self-attention module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a121393a-7b02-4090-b324-4c5ed89d492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    One head of self-attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        # `tril` is a \"buffer\", i.e. it's not a parameter of the module.\n",
    "        # We have to call register_buffer on it.\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        \n",
    "        # Compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        \n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        \n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v     # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e74919-147a-46a5-904e-ab204eb54f96",
   "metadata": {},
   "source": [
    "### Updating our model\n",
    "\n",
    "This is built on our `BigramLanguageModel`, but since it has ceased to be a bigram model, we will call it `GPTLanguageModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "abcc182a-c957-4fce-9b1d-5fa8bcb0378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce new variable: number of embedding dimensions\n",
    "n_embd = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5be99d5b-e938-419d-a017-0c7dd390a4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = vocab_size = 83\n",
      "B = batch_size = 32 = how many independent sequences are being processed at once\n",
      "T = time = length of the running sequence\n",
      "C = channel = 32 = size of the feature vector at each position = embedding dimension\n",
      "** C will no longer be equal to vocab_size; it will be n_embd instead **\n",
      "n_embd = 32 = number of embedding dimensions\n"
     ]
    }
   ],
   "source": [
    "# Some notes on dimensions again\n",
    "print(f\"n = vocab_size = {vocab_size}\")\n",
    "print(f\"B = batch_size = {batch_size} = how many independent sequences are being processed at once\")\n",
    "print(f\"T = time = length of the running sequence\")\n",
    "print(f\"C = channel = {n_embd} = size of the feature vector at each position = embedding dimension\")\n",
    "print(f\"** C will no longer be equal to vocab_size; it will be n_embd instead **\")\n",
    "print(f\"n_embd = {n_embd} = number of embedding dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db9b8224-ab1e-4687-82b1-211a7b302dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Create \"embedding\" table.\n",
    "        # - Maps each token in the vocabulary to an embedding of dimension `n_embd`\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "        # New: add position embedding table\n",
    "        # - Each position in the block gets its own embedding vector\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        # New: add linear layer between embeddings (of dimension `n_embd`)\n",
    "        # and the logits (dimension `vocab_size`)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # New: self-attention head\n",
    "        self.sa_head = Head(n_embd)\n",
    "\n",
    "    def forward(self, indices, targets=None):\n",
    "        # `indices` and `targets` are both (B,T) tensor of integers,\n",
    "        B, T = indices.shape\n",
    "\n",
    "        # For each idx in `indices`, we need to fetch its corresponding logits:\n",
    "        \n",
    "        # (1) New: for each idx in `indices`, we first fetch its embedding\n",
    "        token_emb = self.token_embedding_table(indices) # (B,T,C)\n",
    "        \n",
    "        # (2) New: create the position embedding for each position in the block\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T)) # (T,C)\n",
    "\n",
    "        # (3) New: add the token embedding to position embedding\n",
    "        # - this basically means we add the embedding for each idx in `indices` to\n",
    "        #   the position embedding for its position in the block\n",
    "        # - note the dimension broadcasting here\n",
    "        x = token_emb + pos_emb  # (B,T,C)\n",
    "\n",
    "        # (4) New: apply one head of self-attention\n",
    "        x = self.sa_head(x)      # (B,T,C)\n",
    "        \n",
    "        # (5) New: we then fetch the logits using the lm_head layer\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # We want to flatten `logits` so that we have a total of B*T\n",
    "            # feature vectors of length C.\n",
    "            logits = logits.view(B*T, C)\n",
    "\n",
    "            # Also flatten `targets` so that it contains B*T target outputs\n",
    "            # for each of the feature vectors in `logits`.\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, indices, max_new_tokens):\n",
    "        # `indices` is a (B,T) tensor of indices in the current context\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # New: we need to crop the context; otherwise it won't\n",
    "            # fit into our position_embedding_table\n",
    "            indices_cropped = indices[:, -block_size:]\n",
    "            \n",
    "            # Get predictions;\n",
    "            # `logits` is (B,T,C)\n",
    "            logits, loss = self(indices_cropped) # calls forward()\n",
    "\n",
    "            # `logits` contains the logits for every index in `indices`,\n",
    "            # but we actually only need the last time step in each batch\n",
    "            logits = logits[:, -1, :] # becomes (B,C)\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "\n",
    "            # Sample from the probability distribution\n",
    "            next_idx = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "\n",
    "            # Append sampled index to the context for each batch\n",
    "            indices = torch.cat((indices, next_idx), dim=1) # (B,T+1)\n",
    "\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce7c722-3af0-4a1b-9919-594ea062f733",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57a4167f-dd34-40b6-9091-af7a51f010a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = GPTLanguageModel(vocab_size)\n",
    "\n",
    "# PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e48b8c80-1f8e-4856-ace7-9efdf1d81871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 1: 4.422560214996338\n",
      "Loss at step 5000: 2.260261058807373\n",
      "Loss at step 10000: 2.5313291549682617\n",
      "Loss at step 15000: 2.241060495376587\n",
      "Loss at step 20000: 2.279775381088257\n",
      "Loss at step 25000: 2.36753249168396\n",
      "Loss at step 30000: 2.323002338409424\n",
      "Loss at step 35000: 2.1705868244171143\n",
      "Loss at step 40000: 2.1151444911956787\n",
      "Loss at step 45000: 2.1987550258636475\n",
      "Loss at step 50000: 2.2804505825042725\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Train for some iterations\n",
    "iterations = 50000\n",
    "print_interval = 5000\n",
    "for step in range(iterations):\n",
    "    # Sample a batch of data\n",
    "    xx, yy = get_batch('train', batch_size)\n",
    "\n",
    "    # Evaluate loss\n",
    "    logits, loss = m(xx, yy)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step == 0 or step == iterations-1 or (step+1) % print_interval == 0:\n",
    "        print(f\"Loss at step {step+1}: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85f750f-8e5a-43fb-ab2b-90ca691b0f04",
   "metadata": {},
   "source": [
    "### Generate some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aff969f5-12d6-466b-be98-ee62dbea5d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated:\n",
      "\n",
      "Mire veracnt poinot hanthan?—cerisa thilooff faind rofand ther!” se\n",
      "vo dr agid wely and sth ccam cthad whit do\n",
      "utte, ur.”\n",
      "\n",
      "Mit hawis ay silet frean fo obeedive ononf sito the’s.—She,\n",
      "Mr.\n",
      "\n",
      "\n",
      "wanwe tes hot ind t\n",
      "gthatencond cimbeea Mr efeel\n",
      "aber hes wa, in inout\n",
      "and junconqua out owothen yer! I Herarse sat _he derant s it odu sereand Msreder esh fto whour line lfu men an ethe selil, woro,\n",
      "bered ando yere hallld d me,” fivas che nd rint will onowsey merallewa ttheal—nay on igfe rsa ouldins. Weeaeneq\n"
     ]
    }
   ],
   "source": [
    "# Generate some output, starting with [0]\n",
    "gen = m.generate(indices = torch.zeros((1,1), dtype=torch.long), max_new_tokens=500)\n",
    "print(\"Generated:\")\n",
    "print(decode(gen[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3139825b-8910-4ca9-9302-c0c53c967434",
   "metadata": {},
   "source": [
    "## Adding multi-head self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6978ea86-a3a4-427d-b966-a7ddc91b812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    One head of self-attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        # `tril` is a \"buffer\", i.e. it's not a parameter of the module.\n",
    "        # We have to call register_buffer on it.\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        \n",
    "        # Compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "        \n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        \n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v     # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ec81a84c-295c-4ec7-83d2-37e763afef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple heads of self-attention in parallel.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        # Create multiple heads\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Concatenate the result of each head\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1) # concatenate over the channel dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0dc91713-fd70-48fe-8496-7f16eaba48ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Create \"embedding\" table.\n",
    "        # - Maps each token in the vocabulary to an embedding of dimension `n_embd`\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "        # Add position embedding table\n",
    "        # - Each position in the block gets its own embedding vector\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        # Add linear layer between embeddings (of dimension `n_embd`)\n",
    "        # and the logits (dimension `vocab_size`)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # New: self-attention heads\n",
    "        # i.e. 4 heads of 8-dimensional self-attention\n",
    "        self.sa_heads = MultiHeadAttention(4, n_embd // 4)\n",
    "\n",
    "    def forward(self, indices, targets=None):\n",
    "        # `indices` and `targets` are both (B,T) tensor of integers\n",
    "        B, T = indices.shape\n",
    "\n",
    "        # For each idx in `indices`, we need to fetch its corresponding logits:\n",
    "        \n",
    "        # (1) For each idx in `indices`, we first fetch its embedding\n",
    "        token_emb = self.token_embedding_table(indices) # (B,T,C)\n",
    "        \n",
    "        # (2) Create the position embedding for each position in the block\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T)) # (T,C)\n",
    "\n",
    "        # (3) Add the token embedding to position embedding\n",
    "        # - this basically means we add the embedding for each idx in `indices` to\n",
    "        #   the position embedding for its position in the block\n",
    "        # - note the dimension broadcasting here\n",
    "        x = token_emb + pos_emb  # (B,T,C)\n",
    "\n",
    "        # (4) New: apply multi-head self-attention\n",
    "        x = self.sa_heads(x)      # (B,T,C)\n",
    "        \n",
    "        # (5) We then fetch the logits using the lm_head layer\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # We want to flatten `logits` so that we have a total of B*T\n",
    "            # feature vectors of length C.\n",
    "            logits = logits.view(B*T, C)\n",
    "\n",
    "            # Also flatten `targets` so that it contains B*T target outputs\n",
    "            # for each of the feature vectors in `logits`.\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, indices, max_new_tokens):\n",
    "        # `indices` is a (B,T) tensor of indices in the current context\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # We need to crop the context; otherwise it won't\n",
    "            # fit into our position_embedding_table\n",
    "            indices_cropped = indices[:, -block_size:]\n",
    "            \n",
    "            # Get predictions;\n",
    "            # `logits` is (B,T,C)\n",
    "            logits, loss = self(indices_cropped) # calls forward()\n",
    "\n",
    "            # `logits` contains the logits for every index in `indices`,\n",
    "            # but we actually only need the last time step in each batch\n",
    "            logits = logits[:, -1, :] # becomes (B,C)\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "\n",
    "            # Sample from the probability distribution\n",
    "            next_idx = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "\n",
    "            # Append sampled index to the context for each batch\n",
    "            indices = torch.cat((indices, next_idx), dim=1) # (B,T+1)\n",
    "\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1915ae6-8833-4f33-a10d-a7b4ce52963f",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "905f9c5b-f059-40e3-9a8e-4a6599bab049",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = GPTLanguageModel(vocab_size)\n",
    "\n",
    "# PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "73a1e616-c175-4fcf-a9ed-ffd1b7f76b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 1: 4.453753471374512\n",
      "Loss at step 5000: 2.2575976848602295\n",
      "Loss at step 10000: 2.086601972579956\n",
      "Loss at step 15000: 2.221362829208374\n",
      "Loss at step 20000: 2.1340878009796143\n",
      "Loss at step 25000: 2.0068166255950928\n",
      "Loss at step 30000: 1.95281183719635\n",
      "Loss at step 35000: 1.8485158681869507\n",
      "Loss at step 40000: 1.9005964994430542\n",
      "Loss at step 45000: 2.0300536155700684\n",
      "Loss at step 50000: 2.085979461669922\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Train for some iterations\n",
    "iterations = 50000\n",
    "print_interval = 5000\n",
    "for step in range(iterations):\n",
    "    # Sample a batch of data\n",
    "    xx, yy = get_batch('train', batch_size)\n",
    "\n",
    "    # Evaluate loss\n",
    "    logits, loss = m(xx, yy)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step == 0 or step == iterations-1 or (step+1) % print_interval == 0:\n",
    "        print(f\"Loss at step {step+1}: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd36270b-cba7-4fcd-8bfa-05a44656e3a7",
   "metadata": {},
   "source": [
    "### Generate some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e4df8af8-51ce-4b38-8db0-0703b8d1fdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated:\n",
      "\n",
      "willed.”\n",
      "\n",
      "“You might mort. Wet hour,\n",
      "sholuld.\n",
      "\n",
      "Them consicould fan ligh it the chiet a munt havior a mon!\n",
      "such her dif frome for not his it non dight ken\n",
      "the Hartull, and you\n",
      "aboketfion fa, I had woul holds medead stlaoken’s wifeland at is a cart!\n",
      "\n",
      "pay forettlec with be andiinding her, a hat all sought crapper was lind, and bebkem. Your eveir, thad sencetleld himuch tup i very the father bodived a be tre wreet’s whobut se, bout in Batiettrighice sto-tate ashm.\n",
      "\n",
      "\n",
      "“OLI had in surchy Mrs. Wing of M\n"
     ]
    }
   ],
   "source": [
    "# Generate some output, starting with [0]\n",
    "gen = m.generate(indices = torch.zeros((1,1), dtype=torch.long), max_new_tokens=500)\n",
    "print(\"Generated:\")\n",
    "print(decode(gen[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d452d1cb-adcf-4ac0-82a0-38eaf742991d",
   "metadata": {},
   "source": [
    "## Adding feed-forward layer (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d29cbf93-1272-49e9-b406-1dd0d3afd5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\"\n",
    "    A linear layer followed by a non-linearity\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd), # linear layer\n",
    "            nn.ReLU()                  # non-linearity\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4091aa4e-1077-43d9-a8f5-f78679a5e994",
   "metadata": {},
   "source": [
    "### Transformer block: self-attention + feed-forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ccf7bed4-0aaf-4982-95c7-f7d5d3554064",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer block:\n",
    "    communication (self-attention) followed by computation (feed-forward)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sa(self.ln1(x))\n",
    "        x = self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7ac37964-a0d9-4a66-9990-768cd4378e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Create \"embedding\" table.\n",
    "        # - Maps each token in the vocabulary to an embedding of dimension `n_embd`\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "        # Add position embedding table\n",
    "        # - Each position in the block gets its own embedding vector\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        # Add linear layer between embeddings (of dimension `n_embd`)\n",
    "        # and the logits (dimension `vocab_size`)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # New: add transformer blocks\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "        )\n",
    "\n",
    "    def forward(self, indices, targets=None):\n",
    "        # `indices` and `targets` are both (B,T) tensor of integers\n",
    "        B, T = indices.shape\n",
    "\n",
    "        # For each idx in `indices`, we need to fetch its corresponding logits:\n",
    "        \n",
    "        # (1) For each idx in `indices`, we first fetch its embedding\n",
    "        token_emb = self.token_embedding_table(indices) # (B,T,C)\n",
    "        \n",
    "        # (2) Create the position embedding for each position in the block\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T)) # (T,C)\n",
    "\n",
    "        # (3) Add the token embedding to position embedding\n",
    "        # - this basically means we add the embedding for each idx in `indices` to\n",
    "        #   the position embedding for its position in the block\n",
    "        # - note the dimension broadcasting here\n",
    "        x = token_emb + pos_emb  # (B,T,C)\n",
    "\n",
    "        # (4) New: apply transformer block: self-attention + feed-forward\n",
    "        x = self.blocks(x)       # (B,T,C)\n",
    "        \n",
    "        # (5) We then fetch the logits using the lm_head layer\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # We want to flatten `logits` so that we have a total of B*T\n",
    "            # feature vectors of length C.\n",
    "            logits = logits.view(B*T, C)\n",
    "\n",
    "            # Also flatten `targets` so that it contains B*T target outputs\n",
    "            # for each of the feature vectors in `logits`.\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, indices, max_new_tokens):\n",
    "        # `indices` is a (B,T) tensor of indices in the current context\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # We need to crop the context; otherwise it won't\n",
    "            # fit into our position_embedding_table\n",
    "            indices_cropped = indices[:, -block_size:]\n",
    "            \n",
    "            # Get predictions;\n",
    "            # `logits` is (B,T,C)\n",
    "            logits, loss = self(indices_cropped) # calls forward()\n",
    "\n",
    "            # `logits` contains the logits for every index in `indices`,\n",
    "            # but we actually only need the last time step in each batch\n",
    "            logits = logits[:, -1, :] # becomes (B,C)\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "\n",
    "            # Sample from the probability distribution\n",
    "            next_idx = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "\n",
    "            # Append sampled index to the context for each batch\n",
    "            indices = torch.cat((indices, next_idx), dim=1) # (B,T+1)\n",
    "\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ad2b9-024a-4932-aaad-1c7d6bb25b12",
   "metadata": {},
   "source": [
    "Above won't give very result because we are getting to a pretty deep neural network that suffers from optimization issues.\n",
    "\n",
    "We introduce two optimizations to help with the depth:\n",
    "- residual block: \"ADD\"\n",
    "- layer normalization: \"NORM\"\n",
    "\n",
    "Also: dropout layer\n",
    "- prevents neural net from overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c1f694-08e9-4cae-a775-b3bc26aa617b",
   "metadata": {},
   "source": [
    "### Optimizations for the transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ad41ba3e-25fd-4408-a7d4-396a1825396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.0\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    One head of self-attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        # `tril` is a \"buffer\", i.e. it's not a parameter of the module.\n",
    "        # We have to call register_buffer on it.\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # New: dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        \n",
    "        # Compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "\n",
    "        # New: dropout layer\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v     # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d9b5aa93-a343-4a58-8037-686f60c13223",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple heads of self-attention in parallel.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "        # New: projection\n",
    "        # - it will mix/weigh the outputs from each head\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "        # New: dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Each head outputs (B, T, head_size)\n",
    "        # After torch.cat: out = (B, T, n_head * head_size) = (B, T, n_embd)\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        # New: projection layer\n",
    "        out = self.proj(out)\n",
    "        # New: dropout layer\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cbe9898d-276e-40da-beea-0981a0257779",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\"\n",
    "    A linear layer followed by a non-linearity\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd), # linear layer\n",
    "            nn.ReLU(),                     # non-linearity\n",
    "            nn.Linear(4 * n_embd, n_embd), # New: projection layer\n",
    "            nn.Dropout(dropout),           # New: dropout layer\n",
    "\n",
    "            # Also note the multipler of 4: this is to follow the attention paper\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "148508d0-2d29-4261-8e86-58fce207845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer block:\n",
    "    communication (self-attention) followed by computation (feed-forward)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "\n",
    "        # New: LayerNorm\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,T,C)\n",
    "        # recall that C = n_embd\n",
    "        x = x + self.sa(self.ln1(x))     # residual block: introduce addition with x\n",
    "        x = x + self.ffwd(self.ln2(x))   # residual block: introduce ddition with x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "95f2bf52-acac-405e-8e24-962b1db3563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of transformer blocks\n",
    "n_layer = 4\n",
    "\n",
    "# Number of self-attention heads\n",
    "n_head = 4\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Create \"embedding\" table.\n",
    "        # - Maps each token in the vocabulary to an embedding of dimension `n_embd`\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "        # Add position embedding table\n",
    "        # - Each position in the block gets its own embedding vector\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        # Add linear layer between embeddings (of dimension `n_embd`)\n",
    "        # and the logits (dimension `vocab_size`)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # Add transformer blocks\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "\n",
    "        # New: LayerNorm\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "\n",
    "    def forward(self, indices, targets=None):\n",
    "        # `indices` and `targets` are both (B,T) tensor of integers\n",
    "        B, T = indices.shape\n",
    "\n",
    "        # For each idx in `indices`, we need to fetch its corresponding logits:\n",
    "        \n",
    "        # (1) For each idx in `indices`, we first fetch its embedding\n",
    "        token_emb = self.token_embedding_table(indices) # (B,T,C)\n",
    "        \n",
    "        # (2) Create the position embedding for each position in the block\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T)) # (T,C)\n",
    "\n",
    "        # (3) Add the token embedding to position embedding\n",
    "        # - this basically means we add the embedding for each idx in `indices` to\n",
    "        #   the position embedding for its position in the block\n",
    "        # - note the dimension broadcasting here\n",
    "        x = token_emb + pos_emb  # (B,T,C)\n",
    "\n",
    "        # (4) New: apply transformer block: self-attention + feed-forward\n",
    "        x = self.blocks(x)       # (B,T,C)\n",
    "\n",
    "        # (5) New: apply LayerNorm\n",
    "        x = self.ln_f(x)         # (B,T,C)\n",
    "        \n",
    "        # (6) We then fetch the logits using the lm_head layer\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # We want to flatten `logits` so that we have a total of B*T\n",
    "            # feature vectors of length C.\n",
    "            logits = logits.view(B*T, C)\n",
    "\n",
    "            # Also flatten `targets` so that it contains B*T target outputs\n",
    "            # for each of the feature vectors in `logits`.\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, indices, max_new_tokens):\n",
    "        # `indices` is a (B,T) tensor of indices in the current context\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # We need to crop the context; otherwise it won't\n",
    "            # fit into our position_embedding_table\n",
    "            indices_cropped = indices[:, -block_size:]\n",
    "            \n",
    "            # Get predictions;\n",
    "            # `logits` is (B,T,C)\n",
    "            logits, loss = self(indices_cropped) # calls forward()\n",
    "\n",
    "            # `logits` contains the logits for every index in `indices`,\n",
    "            # but we actually only need the last time step in each batch\n",
    "            logits = logits[:, -1, :] # becomes (B,C)\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "\n",
    "            # Sample from the probability distribution\n",
    "            next_idx = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "\n",
    "            # Append sampled index to the context for each batch\n",
    "            indices = torch.cat((indices, next_idx), dim=1) # (B,T+1)\n",
    "\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ead7d51-829e-4f68-b0a5-cb0f06876cc3",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f0d75267-1713-4f54-be0e-1a29835e450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = GPTLanguageModel(vocab_size)\n",
    "\n",
    "# PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c4aaeaae-39a2-4d14-82ab-cc3c09f64b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 1: 4.669200420379639\n",
      "Loss at step 5000: 1.7074384689331055\n",
      "Loss at step 10000: 1.7004404067993164\n",
      "Loss at step 15000: 1.6498773097991943\n",
      "Loss at step 20000: 1.713290810585022\n",
      "Loss at step 25000: 1.658354640007019\n",
      "Loss at step 30000: 1.7331057786941528\n",
      "Loss at step 35000: 1.7596385478973389\n",
      "Loss at step 40000: 1.6325125694274902\n",
      "Loss at step 45000: 1.7235300540924072\n",
      "Loss at step 50000: 1.6771314144134521\n",
      "Loss at step 55000: 1.6490967273712158\n",
      "Loss at step 60000: 1.6554733514785767\n",
      "Loss at step 65000: 1.656504511833191\n",
      "Loss at step 70000: 1.5700922012329102\n",
      "Loss at step 75000: 1.5170801877975464\n",
      "Loss at step 80000: 1.5542258024215698\n",
      "Loss at step 85000: 1.5349010229110718\n",
      "Loss at step 90000: 1.6250503063201904\n",
      "Loss at step 95000: 1.6151597499847412\n",
      "Loss at step 100000: 1.5942085981369019\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Train for some iterations\n",
    "iterations = 100000\n",
    "print_interval = 5000\n",
    "for step in range(iterations):\n",
    "    # Sample a batch of data\n",
    "    xx, yy = get_batch('train', batch_size)\n",
    "\n",
    "    # Evaluate loss\n",
    "    logits, loss = m(xx, yy)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step == 0 or step == iterations-1 or (step+1) % print_interval == 0:\n",
    "        print(f\"Loss at step {step+1}: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e9f359-f1ac-49ed-a25d-a72f939d365a",
   "metadata": {},
   "source": [
    "### Generate some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "08a2e992-cae6-4126-bcf6-b879830428c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated:\n",
      "\n",
      "justicular\n",
      "imaginell the Frank\n",
      "Colong.\n",
      "Hence, you nother best, by the agreably extralled, and\n",
      "seceshamed Jane Fairfax at all only thoughth, it was scruptions, and _weather passionable that I am pain to you this declination?”—and worth him spiritéing servaking away, and she did I should never\n",
      "most desome once bethlunted it drequest anxion of\n",
      "farther, who concew any recoxcepte,\n",
      "in steased, one brought,” createst-Miss Bates’s were slight conshaded\n",
      "what is quite\n",
      "from any body’s one bring? who half in the great he kind; and\n",
      "solution, that to\n",
      "Mr. Elton issurple, his middles than understanding. He\n",
      "had\n",
      "only living us. She few the little, you short placle some\n",
      "days could her own used him; and all there\n",
      "is quite for so.\n",
      "Colongut sometingsfore her\n",
      "most to be disappoitations what is her\n",
      "father’s enjictely did spodering the nobody that she disgue, Jane, if he us, suffulnuicate Box to convise present it thought another. Mr. Cole, which she than Harreated to\n",
      "her worthing\n",
      "advarer.”\n",
      "\n",
      "“She safely, and c\n"
     ]
    }
   ],
   "source": [
    "# Generate some output, starting with [0]\n",
    "gen = m.generate(indices = torch.zeros((1,1), dtype=torch.long), max_new_tokens=1000)\n",
    "print(\"Generated:\")\n",
    "print(decode(gen[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d219b2-9ef5-438d-bcb7-73aba14a19cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
